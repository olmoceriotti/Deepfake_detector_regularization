{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn2l8fBMjDl-"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-frEmL9rjKp_"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rES557oyiTBm"
      },
      "outputs": [],
      "source": [
        "!pip install torchattacks --quiet\n",
        "!pip install timm --quiet\n",
        "!pip install wandb --quiet\n",
        "!pip install scikit-learn seaborn --quiet\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr6YSAafjPj-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "import torchattacks\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import wandb\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix, accuracy_score\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtglEsUBkuRU"
      },
      "outputs": [],
      "source": [
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "dataset_path = None\n",
        "PREPROCESSED_DATA_PATH = None\n",
        "ROOT_MODEL_DIR = None\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Google Colab environment.\")\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "        BASE_DRIVE_PATH = '/content/drive/MyDrive'\n",
        "        CV_PROJECT_FOLDER_COLAB = os.path.join(BASE_DRIVE_PATH, 'CV')\n",
        "        dataset_path = os.path.join(CV_PROJECT_FOLDER_COLAB, 'Dataset')\n",
        "        PREPROCESSED_DATA_PATH = os.path.join(CV_PROJECT_FOLDER_COLAB, 'Dataset_pickle')\n",
        "        ROOT_MODEL_DIR = os.path.join(CV_PROJECT_FOLDER_COLAB, 'models')\n",
        "\n",
        "        os.makedirs(dataset_path, exist_ok=True)\n",
        "        os.makedirs(PREPROCESSED_DATA_PATH, exist_ok=True)\n",
        "\n",
        "        print(f\"Colab raw data path set to: {dataset_path}\")\n",
        "        print(f\"Colab preprocessed data (.pkl) path set to: {PREPROCESSED_DATA_PATH}\")\n",
        "        print(f\"Colab model save/load (.pth) path set to: {ROOT_MODEL_DIR}\")\n",
        "\n",
        "        print(\"Attempting W&B login for Colab...\")\n",
        "        wandb.login()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Error: google.colab module not found, but IN_COLAB was true. This is unexpected.\")\n",
        "        sys.exit(\"Colab environment detection mismatch.\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error during Colab drive mount: {e}\")\n",
        "        sys.exit(\"Colab environment detected, but drive mount failed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Colab setup: {e}\")\n",
        "        sys.exit(\"Colab setup failed.\")\n",
        "\n",
        "else:\n",
        "    print(\"Not in Colab, setting up for local execution.\")\n",
        "    ROOT_DIR_LOCAL = \"./\"\n",
        "\n",
        "    dataset_path = os.path.join(ROOT_DIR_LOCAL, \"data\", \"Dataset_Raw\")\n",
        "    PREPROCESSED_DATA_PATH = os.path.join(ROOT_DIR_LOCAL, \"data\", \"Dataset_Preprocessed\")\n",
        "    ROOT_MODEL_DIR = os.path.join(ROOT_DIR_LOCAL, \"models\")\n",
        "\n",
        "    os.makedirs(dataset_path, exist_ok=True)\n",
        "    os.makedirs(PREPROCESSED_DATA_PATH, exist_ok=True)\n",
        "    os.makedirs(ROOT_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "    print(f\"Local raw data path set to: {dataset_path}\")\n",
        "    print(f\"Local preprocessed data (.pkl) path set to: {PREPROCESSED_DATA_PATH}\")\n",
        "    print(f\"Local model save/load (.pth) path set to: {ROOT_MODEL_DIR}\")\n",
        "\n",
        "    print(\"Not in Colab, wandb.login() skipped. Ensure you are logged in via CLI (e.g., 'wandb login').\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if dataset_path is None or PREPROCESSED_DATA_PATH is None or ROOT_MODEL_DIR is None:\n",
        "    sys.exit(\"Critical error: Root directory paths were not set. Please check the setup logic.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32n0xoSvjYUu"
      },
      "source": [
        "## Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GDbfsjsigWf"
      },
      "outputs": [],
      "source": [
        "model_name_flag = 'efficientnet' #{'efficientnet', 'efficientnet_pim', 'efficientnet_freq', 'efficientnet_adv'}\n",
        "epochs = 5\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_name_flag = 'adamw'\n",
        "\n",
        "R_PIM = 0.1\n",
        "ALPHA_PIM = 1.0\n",
        "SHALLOW_FEATURE_IDX = 2\n",
        "\n",
        "num_workers = 0 if os.name == 'nt' else 2\n",
        "\n",
        "WANDB_PROJECT_NAME = \"CV_Project_Adversarial_DeepFake\"\n",
        "WANDB_ENTITY = \"olmoceriotti\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5h0So51CD0b"
      },
      "outputs": [],
      "source": [
        "true_dataset_path = os.path.join(dataset_path, 'true_dataset')\n",
        "fake_dataset_path = os.path.join(dataset_path, 'fake_dataset_one_source')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9KseWpOjmRP"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9HSLaFhijqU"
      },
      "outputs": [],
      "source": [
        "def init_train_model(model_name_str, opt_name_str, lr,\n",
        "                     r_pim_hp=R_PIM, alpha_pim_hp=ALPHA_PIM, shallow_idx_hp=SHALLOW_FEATURE_IDX,\n",
        "                     adv_drop_prob_hp=0.1, adv_block_size_hp=7, adv_classifier_dropout_hp=0.3,\n",
        "                     smooth_sigma_hp=0.25, smooth_num_samples_hp=5):\n",
        "    if model_name_str == 'efficientnet':\n",
        "        weights = models.EfficientNet_B0_Weights.IMAGENET1K_V1\n",
        "        model = models.efficientnet_b0(weights=weights)\n",
        "        num_ftrs = model.classifier[1].in_features\n",
        "        model.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.2, inplace=True),\n",
        "            nn.Linear(num_ftrs, 2)\n",
        "        )\n",
        "    elif model_name_str == 'efficientnet_pim':\n",
        "        model = EfficientNetB0WithPIM(pretrained=True, num_classes=2, shallow_feature_idx=shallow_idx_hp)\n",
        "    elif model_name_str == 'efficientnet_freq':\n",
        "        model = get_efficientnet_freq()\n",
        "    elif model_name_str == 'efficientnet_adv':\n",
        "        model = EfficientNetB0WithSpatialDropBlock(\n",
        "            num_classes=2,\n",
        "            drop_prob=adv_drop_prob_hp,\n",
        "            block_size=adv_block_size_hp,\n",
        "            classifier_dropout=adv_classifier_dropout_hp\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model name: {model_name_str}\")\n",
        "\n",
        "    if opt_name_str == 'adamw':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    elif opt_name_str == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    elif opt_name_str == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    else:\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    return model, optimizer\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in tqdm(dataloader, desc=\"Training (Standard)\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    if total == 0: return 0.0, 0.0\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device, return_preds_labels=False):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels_list = []\n",
        "    all_preds_probs_list = []\n",
        "    all_preds_labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            if return_preds_labels:\n",
        "                all_labels_list.append(labels.cpu())\n",
        "                all_preds_probs_list.append(probs.cpu())\n",
        "                all_preds_labels_list.append(predicted.cpu())\n",
        "\n",
        "    if total == 0:\n",
        "        if return_preds_labels:\n",
        "            return 0.0, 0.0, torch.empty(0), torch.empty(0), torch.empty(0)\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    avg_acc = correct / total\n",
        "\n",
        "    if return_preds_labels:\n",
        "        all_labels = torch.cat(all_labels_list)\n",
        "        all_preds_probs = torch.cat(all_preds_probs_list)\n",
        "        all_preds_labels = torch.cat(all_preds_labels_list)\n",
        "        return avg_loss, avg_acc, all_labels, all_preds_probs, all_preds_labels\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "\n",
        "def train_one_epoch_with_pim(model, dataloader, optimizer, criterion, device, alpha_pim_val, r_pim_val):\n",
        "    model.train()\n",
        "    running_loss_objective = 0.0\n",
        "    correct_empirical = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, labels in tqdm(dataloader, desc=\"Training with PIM\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        batch_size = images.size(0)\n",
        "        total_samples += batch_size\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        model.is_first_pass = True\n",
        "\n",
        "        outputs_empirical = model(images)\n",
        "\n",
        "        if model.mu_shallow is None or model.sigma_shallow is None:\n",
        "            print(\"Warning: mu_shallow or sigma_shallow not populated in first pass. Skipping PIM for this batch.\")\n",
        "            loss_empirical_fallback = criterion(outputs_empirical, labels)\n",
        "            loss_empirical_fallback.backward()\n",
        "            optimizer.step()\n",
        "            running_loss_objective += loss_empirical_fallback.item() * batch_size\n",
        "            _, predicted_emp_fb = torch.max(outputs_empirical.data, 1)\n",
        "            correct_empirical += (predicted_emp_fb == labels).sum().item()\n",
        "            continue\n",
        "\n",
        "        model.mu_shallow.retain_grad()\n",
        "        model.sigma_shallow.retain_grad()\n",
        "\n",
        "        loss_empirical = criterion(outputs_empirical, labels)\n",
        "        loss_empirical.backward(retain_graph=True)\n",
        "\n",
        "        g1_theta_grads = {name: param.grad.clone() for name, param in model.named_parameters() if param.requires_grad and param.grad is not None}\n",
        "\n",
        "        grad_mu_s_batch_specific = model.mu_shallow.grad if model.mu_shallow.grad is not None else torch.zeros_like(model.mu_shallow)\n",
        "        grad_sigma_s_batch_specific = model.sigma_shallow.grad if model.sigma_shallow.grad is not None else torch.zeros_like(model.sigma_shallow)\n",
        "\n",
        "        grad_mu_s_batch_level = grad_mu_s_batch_specific.mean(dim=0, keepdim=True)\n",
        "        grad_sigma_s_batch_level = grad_sigma_s_batch_specific.mean(dim=0, keepdim=True)\n",
        "\n",
        "        stacked_grads = torch.cat((grad_mu_s_batch_level.flatten(), grad_sigma_s_batch_level.flatten()))\n",
        "        norm_val = torch.norm(stacked_grads, p=2) + 1e-7\n",
        "\n",
        "        delta_mu_batch = r_pim_val * grad_mu_s_batch_level / norm_val\n",
        "        delta_sigma_batch = r_pim_val * grad_sigma_s_batch_level / norm_val\n",
        "\n",
        "        delta_mu_batch = delta_mu_batch.detach()\n",
        "        delta_sigma_batch = delta_sigma_batch.detach()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        model.is_first_pass = False\n",
        "        outputs_regularization = model(images, delta_mu_for_pim=delta_mu_batch, delta_sigma_for_pim=delta_sigma_batch)\n",
        "        loss_regularization = criterion(outputs_regularization, labels)\n",
        "\n",
        "        loss_regularization.backward()\n",
        "        g2_theta_grads = {name: param.grad.clone() for name, param in model.named_parameters() if param.requires_grad and param.grad is not None}\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                grad1 = g1_theta_grads.get(name, torch.zeros_like(param.data))\n",
        "                grad2 = g2_theta_grads.get(name, torch.zeros_like(param.data))\n",
        "\n",
        "                param.grad = (1 - alpha_pim_val) * grad1 + alpha_pim_val * grad2\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss_objective = (1 - alpha_pim_val) * loss_empirical.item() + alpha_pim_val * loss_regularization.item()\n",
        "        running_loss_objective += current_loss_objective * batch_size\n",
        "\n",
        "        _, predicted_emp = torch.max(outputs_empirical.data, 1)\n",
        "        correct_empirical += (predicted_emp == labels).sum().item()\n",
        "\n",
        "    if total_samples == 0: return 0.0, 0.0\n",
        "    avg_loss_objective = running_loss_objective / total_samples\n",
        "    avg_acc_empirical = correct_empirical / total_samples\n",
        "    return avg_loss_objective, avg_acc_empirical\n",
        "\n",
        "def evaluate_on_adversarial(model_to_eval, test_dataloader, loss_criterion, current_device,\n",
        "                            attack_instance, attack_display_name, return_details=False):\n",
        "    model_to_eval.eval()\n",
        "    adv_correct_count = 0\n",
        "    adv_total_samples = 0\n",
        "    adv_cumulative_loss = 0.0\n",
        "    progress_bar_desc = f\"Attacking with {attack_display_name}\"\n",
        "\n",
        "    all_orig_labels_list = []\n",
        "    all_adv_preds_probs_list = []\n",
        "    all_adv_preds_labels_list = []\n",
        "    total_perturbation_l0 = 0.0\n",
        "    total_perturbation_l2 = 0.0\n",
        "    total_perturbation_linf = 0.0\n",
        "    perturbed_sample_count = 0\n",
        "\n",
        "    for images_batch, labels_batch in tqdm(test_dataloader, desc=progress_bar_desc):\n",
        "        images_batch, labels_batch = images_batch.to(current_device), labels_batch.to(current_device)\n",
        "        try:\n",
        "            adv_images_batch = attack_instance(images_batch, labels_batch)\n",
        "        except Exception as e:\n",
        "            try:\n",
        "                adv_images_batch = attack_instance(images_batch)\n",
        "            except Exception as e2:\n",
        "                print(f\"Fallback failed for {attack_display_name}. Error: {e2}. Skipping batch.\")\n",
        "                continue\n",
        "        with torch.no_grad():\n",
        "            outputs_adv = model_to_eval(adv_images_batch)\n",
        "            loss_adv = loss_criterion(outputs_adv, labels_batch)\n",
        "            probs_adv = torch.softmax(outputs_adv, dim=1)\n",
        "            _, predicted_adv = torch.max(outputs_adv.data, 1)\n",
        "\n",
        "            adv_cumulative_loss += loss_adv.item() * images_batch.size(0)\n",
        "            adv_correct_count += (predicted_adv == labels_batch).sum().item()\n",
        "            adv_total_samples += labels_batch.size(0)\n",
        "\n",
        "            if return_details:\n",
        "                all_orig_labels_list.append(labels_batch.cpu())\n",
        "                all_adv_preds_probs_list.append(probs_adv.cpu())\n",
        "                all_adv_preds_labels_list.append(predicted_adv.cpu())\n",
        "\n",
        "                perturbation = adv_images_batch - images_batch\n",
        "                total_perturbation_l0 += perturbation.abs().gt(1e-6).view(images_batch.size(0), -1).sum(dim=1).float().sum()\n",
        "                total_perturbation_l2 += torch.norm(perturbation.view(images_batch.size(0), -1), p=2, dim=1).sum()\n",
        "                total_perturbation_linf += torch.norm(perturbation.view(images_batch.size(0), -1), p=float('inf'), dim=1).sum()\n",
        "                perturbed_sample_count += images_batch.size(0)\n",
        "\n",
        "\n",
        "    if adv_total_samples == 0:\n",
        "        print(f\"Warning: No samples were processed for attack {attack_display_name}.\")\n",
        "        if return_details:\n",
        "            return 0.0, 0.0, torch.empty(0), torch.empty(0), torch.empty(0), 0.0, 0.0, 0.0\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    final_adv_accuracy = adv_correct_count / adv_total_samples\n",
        "    final_adv_avg_loss = adv_cumulative_loss / adv_total_samples\n",
        "\n",
        "    if return_details:\n",
        "        all_orig_labels = torch.cat(all_orig_labels_list) if all_orig_labels_list else torch.empty(0)\n",
        "        all_adv_preds_probs = torch.cat(all_adv_preds_probs_list) if all_adv_preds_probs_list else torch.empty(0)\n",
        "        all_adv_preds_labels = torch.cat(all_adv_preds_labels_list) if all_adv_preds_labels_list else torch.empty(0)\n",
        "        avg_perturb_l0 = (total_perturbation_l0 / perturbed_sample_count).item() if perturbed_sample_count > 0 else 0.0\n",
        "        avg_perturb_l2 = (total_perturbation_l2 / perturbed_sample_count).item() if perturbed_sample_count > 0 else 0.0\n",
        "        avg_perturb_linf = (total_perturbation_linf / perturbed_sample_count).item() if perturbed_sample_count > 0 else 0.0\n",
        "        return final_adv_accuracy, final_adv_avg_loss, all_orig_labels, all_adv_preds_probs, all_adv_preds_labels, avg_perturb_l0, avg_perturb_l2, avg_perturb_linf\n",
        "    return final_adv_accuracy, final_adv_avg_loss\n",
        "\n",
        "def log_detailed_metrics(y_true_tensor, y_pred_probs_tensor, y_pred_labels_tensor, section_prefix=\"test/clean\", log_to_wandb=True):\n",
        "    if y_true_tensor.numel() == 0 or y_pred_labels_tensor.numel() == 0:\n",
        "        print(f\"Skipping detailed metrics for {section_prefix} due to empty tensors.\")\n",
        "        if log_to_wandb:\n",
        "            wandb.log({\n",
        "                f\"{section_prefix}_accuracy\": \"N/A\", f\"{section_prefix}_precision\": \"N/A\",\n",
        "                f\"{section_prefix}_recall\": \"N/A\", f\"{section_prefix}_f1_score\": \"N/A\",\n",
        "                f\"{section_prefix}_auc_roc\": \"N/A\", f\"{section_prefix}_avg_confidence\": \"N/A\"\n",
        "            })\n",
        "        return\n",
        "\n",
        "    y_true_cpu = y_true_tensor.cpu().numpy()\n",
        "    y_pred_labels_cpu = y_pred_labels_tensor.cpu().numpy()\n",
        "    y_pred_probs_cpu = y_pred_probs_tensor.cpu().numpy()\n",
        "\n",
        "    accuracy = accuracy_score(y_true_cpu, y_pred_labels_cpu)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true_cpu, y_pred_labels_cpu, average='binary', zero_division=0)\n",
        "\n",
        "    auc_roc = \"N/A\"\n",
        "    if len(np.unique(y_true_cpu)) > 1 and y_pred_probs_cpu.ndim == 2 and y_pred_probs_cpu.shape[1] == 2:\n",
        "        try:\n",
        "            auc_roc = roc_auc_score(y_true_cpu, y_pred_probs_cpu[:, 1])\n",
        "        except ValueError as e:\n",
        "            print(f\"Could not compute AUC for {section_prefix}: {e}\")\n",
        "            auc_roc = \"N/A\"\n",
        "    elif y_pred_probs_cpu.ndim != 2 or y_pred_probs_cpu.shape[1] != 2:\n",
        "         print(f\"AUC calculation skipped for {section_prefix}: y_pred_probs_cpu has unexpected shape {y_pred_probs_cpu.shape}.\")\n",
        "\n",
        "\n",
        "    cm = confusion_matrix(y_true_cpu, y_pred_labels_cpu)\n",
        "    fig_cm, ax_cm = plt.subplots()\n",
        "    sns.heatmap(cm, annot=True, fmt='d', ax=ax_cm, cmap='Blues', cbar=False)\n",
        "    ax_cm.set_xlabel('Predicted labels')\n",
        "    ax_cm.set_ylabel('True labels')\n",
        "    ax_cm.set_title(f'Confusion Matrix - {section_prefix}')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "    avg_confidence = 0.0\n",
        "    if y_pred_probs_cpu.size > 0:\n",
        "        avg_confidence = y_pred_probs_cpu[np.arange(len(y_pred_labels_cpu)), y_pred_labels_cpu].mean()\n",
        "\n",
        "\n",
        "    metrics_log = {\n",
        "        f\"{section_prefix}_accuracy\": accuracy,\n",
        "        f\"{section_prefix}_precision\": precision,\n",
        "        f\"{section_prefix}_recall\": recall,\n",
        "        f\"{section_prefix}_f1_score\": f1,\n",
        "        f\"{section_prefix}_avg_confidence\": avg_confidence,\n",
        "    }\n",
        "    if auc_roc != \"N/A\":\n",
        "        metrics_log[f\"{section_prefix}_auc_roc\"] = auc_roc\n",
        "\n",
        "    print(f\"\\nDetailed metrics for {section_prefix}:\")\n",
        "    for k, v in metrics_log.items():\n",
        "        if isinstance(v, float): print(f\"  {k}: {v:.4f}\")\n",
        "        else: print(f\"  {k}: {v}\")\n",
        "    print(f\"  Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    if log_to_wandb:\n",
        "        wandb.log(metrics_log)\n",
        "        wandb.log({f\"{section_prefix}_confusion_matrix\": wandb.Image(fig_cm)})\n",
        "    plt.close(fig_cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fTFs7q-jqLU"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJddgcJDkD3F"
      },
      "outputs": [],
      "source": [
        "class DFFDDataset(Dataset):\n",
        "    def __init__(self, true_dir, fake_dir, split='train', transform=None):\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "\n",
        "        true_split_dir = os.path.join(true_dir, split)\n",
        "        fake_split_dir = os.path.join(fake_dir, split)\n",
        "\n",
        "        if not os.path.exists(true_split_dir):\n",
        "            print(f\"Warning: Path does not exist {true_split_dir}\")\n",
        "        else:\n",
        "            for fname in os.listdir(true_split_dir):\n",
        "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    self.image_paths.append(os.path.join(true_split_dir, fname))\n",
        "                    self.labels.append(0)\n",
        "\n",
        "        if not os.path.exists(fake_split_dir):\n",
        "            print(f\"Warning: Path does not exist {fake_split_dir}\")\n",
        "        else:\n",
        "            for fname in os.listdir(fake_split_dir):\n",
        "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    self.image_paths.append(os.path.join(fake_split_dir, fname))\n",
        "                    self.labels.append(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            sys.exit(-1)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def load_dataset(filepath):\n",
        "  with open(filepath, 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "  print(f\"Dataset loaded from {filepath}\")\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xrf3aRlkHxl"
      },
      "outputs": [],
      "source": [
        "class InMemoryDFFDDataset(Dataset):\n",
        "    def __init__(self, true_dir, fake_dir, split='train', transform=None):\n",
        "        self.processed_images = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "\n",
        "        true_split_dir = os.path.join(true_dir, split)\n",
        "        fake_split_dir = os.path.join(fake_dir, split)\n",
        "\n",
        "        print(f\"Loading images from: {true_split_dir} (True) and {fake_split_dir} (Fake)\")\n",
        "\n",
        "        # Load TRUE images\n",
        "        if not os.path.exists(true_split_dir):\n",
        "            print(f\"Warning: Path does not exist {true_split_dir}\")\n",
        "        else:\n",
        "            for fname in os.listdir(true_split_dir):\n",
        "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(true_split_dir, fname)\n",
        "                    try:\n",
        "                        image = Image.open(img_path).convert('RGB')\n",
        "                        if self.transform:\n",
        "                            image = self.transform(image)\n",
        "                        self.processed_images.append(image)\n",
        "                        self.labels.append(0)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading or transforming image {img_path}: {e}\")\n",
        "                        continue\n",
        "\n",
        "        # Load FAKE images\n",
        "        if not os.path.exists(fake_split_dir):\n",
        "            print(f\"Warning: Path does not exist {fake_split_dir}\")\n",
        "        else:\n",
        "            for fname in os.listdir(fake_split_dir):\n",
        "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(fake_split_dir, fname)\n",
        "                    try:\n",
        "                        image = Image.open(img_path).convert('RGB')\n",
        "                        if self.transform:\n",
        "                            image = self.transform(image)\n",
        "                        self.processed_images.append(image)\n",
        "                        self.labels.append(1)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading or transforming image {img_path}: {e}\")\n",
        "                        continue\n",
        "\n",
        "        if not self.processed_images:\n",
        "            print(\"Warning: No images were loaded. Check paths and image files.\")\n",
        "        else:\n",
        "            print(f\"Successfully loaded and processed {len(self.processed_images)} images into memory.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.processed_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.processed_images[idx]\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "    def save(self, filepath):\n",
        "      with open(filepath, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "      print(f\"Dataset saved to {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WusfEJLaimes"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "LOAD_PREPROCESSED_DATASETS = True\n",
        "\n",
        "if LOAD_PREPROCESSED_DATASETS:\n",
        "    try:\n",
        "        train_dataset = load_dataset(os.path.join(PREPROCESSED_DATA_PATH, 'train_dataset_ffhq_stylegan.pkl'))\n",
        "        val_dataset   = load_dataset(os.path.join(PREPROCESSED_DATA_PATH, 'val_dataset_ffhq_stylegan.pkl'))\n",
        "        test_dataset  = load_dataset(os.path.join(PREPROCESSED_DATA_PATH, 'test_dataset_ffhq_stylegan.pkl'))\n",
        "        print(\"Successfully loaded preprocessed datasets.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Preprocessed dataset files not found. Processing from scratch...\")\n",
        "        LOAD_PREPROCESSED_DATASETS = False\n",
        "\n",
        "if not LOAD_PREPROCESSED_DATASETS:\n",
        "    train_dataset = InMemoryDFFDDataset(true_dir=true_dataset_path, fake_dir=fake_dataset_path, split='train', transform=transform)\n",
        "    val_dataset   = InMemoryDFFDDataset(true_dir=true_dataset_path, fake_dir=fake_dataset_path, split='validation', transform=transform)\n",
        "    test_dataset  = InMemoryDFFDDataset(true_dir=true_dataset_path, fake_dir=fake_dataset_path, split='test', transform=transform)\n",
        "\n",
        "    os.makedirs(PREPROCESSED_DATA_PATH, exist_ok=True)\n",
        "    train_dataset.save(os.path.join(PREPROCESSED_DATA_PATH, 'train_dataset_ffhq_stylegan.pkl'))\n",
        "    val_dataset.save(os.path.join(PREPROCESSED_DATA_PATH, 'val_dataset_ffhq_stylegan.pkl'))\n",
        "    test_dataset.save(os.path.join(PREPROCESSED_DATA_PATH, 'test_dataset_ffhq_stylegan.pkl'))\n",
        "\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "if len(train_dataset) == 0 or len(val_dataset) == 0 or len(test_dataset) == 0:\n",
        "    print(\"Warning: One or more datasets are empty. Check dataset paths and structure.\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=(len(train_dataset) > 32 and 'pim' in model_name_flag)) if len(train_dataset) > 0 else None\n",
        "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=num_workers, pin_memory=True,  drop_last=False) if len(val_dataset) > 0 else None # drop_last generally not needed for val/test\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=num_workers, pin_memory=True,  drop_last=False) if len(test_dataset) > 0 else None\n",
        "\n",
        "if train_loader is None or len(train_loader) == 0: print(\"Warning: train_loader is empty or None.\")\n",
        "if val_loader is None or len(val_loader) == 0: print(\"Warning: val_loader is empty or None.\")\n",
        "if test_loader is None or len(test_loader) == 0: print(\"Warning: test_loader is empty or None.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx8qwM1CIBpP"
      },
      "outputs": [],
      "source": [
        "train_dataset.save(os.path.join(PREPROCESSED_DATA_PATH, 'train_dataset_ffhq_stylegan.pkl'))\n",
        "val_dataset.save(os.path.join(PREPROCESSED_DATA_PATH, 'val_dataset_ffhq_stylegan.pkl'))\n",
        "test_dataset.save(os.path.join(PREPROCESSED_DATA_PATH, 'test_dataset_ffhq_stylegan.pkl'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt_rIKOqkYKk"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELzvoShxkZ9T"
      },
      "source": [
        "### Standard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G--s6Ezdkfar"
      },
      "source": [
        "#### EfficientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf2aytzGkkCk"
      },
      "outputs": [],
      "source": [
        "def get_efficientnet(pretrained=True):\n",
        "    weights = models.EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "    model = models.efficientnet_b0(weights=weights)\n",
        "\n",
        "    num_ftrs = model.classifier[1].in_features\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.2, inplace=True),\n",
        "        nn.Linear(num_ftrs, 2)\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUAOms1hknsP"
      },
      "source": [
        "### With regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZcu-mLhizp7"
      },
      "outputs": [],
      "source": [
        "class EfficientNetB0WithPIM(nn.Module):\n",
        "    def __init__(self, pretrained=True, num_classes=2, shallow_feature_idx=2):\n",
        "        super().__init__()\n",
        "        weights = models.EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        self.base_model = models.efficientnet_b0(weights=weights)\n",
        "\n",
        "        if not (0 <= shallow_feature_idx < len(self.base_model.features) -1):\n",
        "            raise ValueError(f\"shallow_feature_idx must be between 0 and {len(self.base_model.features)-2}\")\n",
        "\n",
        "        self.shallow_features_extractor = self.base_model.features[:shallow_feature_idx + 1]\n",
        "        self.deep_features_extractor = nn.Sequential(\n",
        "            *self.base_model.features[shallow_feature_idx + 1:],\n",
        "            self.base_model.avgpool\n",
        "        )\n",
        "\n",
        "        original_classifier = self.base_model.classifier\n",
        "        if isinstance(original_classifier, nn.Sequential) and \\\n",
        "           len(original_classifier) > 1 and \\\n",
        "           isinstance(original_classifier[1], nn.Linear):\n",
        "            num_ftrs = original_classifier[1].in_features\n",
        "            self.classifier = nn.Sequential(\n",
        "                original_classifier[0],\n",
        "                nn.Linear(num_ftrs, num_classes)\n",
        "            )\n",
        "        elif isinstance(original_classifier, nn.Linear):\n",
        "            num_ftrs = original_classifier.in_features\n",
        "            self.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        else:\n",
        "            print(\"Warning: Classifier structure not as expected for EfficientNetB0. Using a generic classifier.\")\n",
        "            dummy_input = torch.randn(1, 3, 224, 224)\n",
        "            with torch.no_grad():\n",
        "                _ = self.base_model.features(dummy_input)\n",
        "                pooled_features = self.base_model.avgpool(_)\n",
        "                flattened_features = torch.flatten(pooled_features, 1)\n",
        "            num_ftrs = flattened_features.shape[1]\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Dropout(p=0.2, inplace=True),\n",
        "                nn.Linear(num_ftrs, num_classes)\n",
        "            )\n",
        "\n",
        "\n",
        "        self.mu_shallow = None\n",
        "        self.sigma_shallow = None\n",
        "        self.is_first_pass = True\n",
        "\n",
        "    def forward(self, x, delta_mu_for_pim=None, delta_sigma_for_pim=None):\n",
        "        s_feats = self.shallow_features_extractor(x)\n",
        "\n",
        "        if self.training and not self.is_first_pass:\n",
        "            if delta_mu_for_pim is None or delta_sigma_for_pim is None:\n",
        "                raise ValueError(\"delta_mu/sigma must be provided for PIM's second pass during training\")\n",
        "            s_feats_norm = (s_feats - self.mu_shallow.detach()) / (self.sigma_shallow.detach() + 1e-5)\n",
        "            perturbed_sigma_val = self.sigma_shallow.detach() + delta_sigma_for_pim\n",
        "            perturbed_mu_val = self.mu_shallow.detach() + delta_mu_for_pim\n",
        "            output_s_feats = s_feats_norm * perturbed_sigma_val + perturbed_mu_val\n",
        "        else:\n",
        "            if self.training and self.is_first_pass:\n",
        "                self.mu_shallow = s_feats.mean(dim=[2, 3], keepdim=True)\n",
        "                self.sigma_shallow = s_feats.std(dim=[2, 3], correction=0, keepdim=True) + 1e-5\n",
        "            output_s_feats = s_feats\n",
        "\n",
        "        d_feats = self.deep_features_extractor(output_s_feats)\n",
        "        d_feats_flat = torch.flatten(d_feats, 1)\n",
        "        output = self.classifier(d_feats_flat)\n",
        "        return output\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        super().train(mode)\n",
        "        if not mode:\n",
        "            self.is_first_pass = True\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8ZfVdCgk1yp"
      },
      "source": [
        "### Other techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lejWhphdk3Wr"
      },
      "outputs": [],
      "source": [
        "class DropBlock(nn.Module):\n",
        "    def __init__(self, drop_prob=0.1, block_size=3):\n",
        "        super(DropBlock, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or self.drop_prob == 0.:\n",
        "            return x\n",
        "\n",
        "        if x.dim() < 4:\n",
        "            raise ValueError(f\"DropBlock expects a 4D input (Batch, Channels, Height, Width), but got {x.dim()}D tensor with shape {x.shape}. \"\n",
        "                             \"Ensure DropBlock is placed before pooling/flattening operations.\")\n",
        "\n",
        "        gamma = self.drop_prob / (self.block_size ** 2)\n",
        "        mask_input_subsampled = x[:, :, ::self.block_size, ::self.block_size]\n",
        "        mask = (torch.rand_like(mask_input_subsampled) < gamma).float()\n",
        "        mask = nn.functional.interpolate(mask, scale_factor=self.block_size, mode='nearest')\n",
        "\n",
        "        if mask.shape[2:] != x.shape[2:]:\n",
        "            mask = mask[:, :, :x.shape[2], :x.shape[3]]\n",
        "        return x * (1 - mask)\n",
        "\n",
        "class EfficientNetB0WithSpatialDropBlock(nn.Module):\n",
        "    def __init__(self, num_classes=2, drop_prob=0.1, block_size=7, classifier_dropout=0.3):\n",
        "        super().__init__()\n",
        "        weights = models.EfficientNet_B0_Weights.IMAGENET1K_V1\n",
        "        base_model = models.efficientnet_b0(weights=weights)\n",
        "\n",
        "        self.features = base_model.features\n",
        "        self.drop_block = DropBlock(drop_prob=drop_prob, block_size=block_size)\n",
        "        self.avgpool = base_model.avgpool\n",
        "\n",
        "        num_ftrs_for_classifier = base_model.classifier[1].in_features\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=classifier_dropout, inplace=True),\n",
        "            nn.Linear(num_ftrs_for_classifier, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.drop_block(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def get_efficientnet_adv():\n",
        "    return EfficientNetB0WithSpatialDropBlock(\n",
        "        num_classes=2,\n",
        "        drop_prob=0.1,\n",
        "        block_size=7,\n",
        "        classifier_dropout=0.3\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8VBjOZpxK2h"
      },
      "outputs": [],
      "source": [
        "class FrequencyLayer(nn.Module):\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 3:\n",
        "            x = x.unsqueeze(0)\n",
        "        fft = torch.fft.fft2(x, dim=(-2, -1))\n",
        "        fft_mag = torch.abs(fft)\n",
        "        return fft_mag\n",
        "\n",
        "class EfficientNetFrequency(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        weights = models.EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        base = models.efficientnet_b0(weights=weights)\n",
        "\n",
        "        self.freq_layer = FrequencyLayer()\n",
        "        self.base_model_features = base.features\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        dummy_input = torch.randn(1, 3, 224, 224)\n",
        "        with torch.no_grad():\n",
        "            features = self.base_model_features(dummy_input)\n",
        "            pooled_features = self.pool(features)\n",
        "        num_ftrs_classifier = pooled_features.shape[1]\n",
        "\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(num_ftrs_classifier, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_freq = self.freq_layer(x)\n",
        "        x = self.base_model_features(x_freq)\n",
        "        x = self.pool(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def get_efficientnet_freq():\n",
        "    return EfficientNetFrequency(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt0BFdonk8ca"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kq8aCpdB5RZ"
      },
      "source": [
        "### Real training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7-01yZIi0W7"
      },
      "outputs": [],
      "source": [
        "run_name = f\"{model_name_flag}_epochs{epochs}_lr{learning_rate}\"\n",
        "if 'pim' in model_name_flag:\n",
        "    run_name += f\"_pim_r{R_PIM}_alpha{ALPHA_PIM}_shallow{SHALLOW_FEATURE_IDX}\"\n",
        "\n",
        "wandb.init(\n",
        "    project=WANDB_PROJECT_NAME,\n",
        "    entity=WANDB_ENTITY,\n",
        "    name=run_name,\n",
        "    config={\n",
        "        \"model_name\": model_name_flag,\n",
        "        \"epochs\": epochs,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"optimizer\": optimizer_name_flag,\n",
        "        \"criterion\": \"CrossEntropyLoss\",\n",
        "        \"R_PIM\": R_PIM if 'pim' in model_name_flag else None,\n",
        "        \"ALPHA_PIM\": ALPHA_PIM if 'pim' in model_name_flag else None,\n",
        "        \"SHALLOW_FEATURE_IDX\": SHALLOW_FEATURE_IDX if 'pim' in model_name_flag else None,\n",
        "        \"dataset_path\": dataset_path,\n",
        "        \"device\": str(device)\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"\\n--- Initializing Model: {model_name_flag} ---\")\n",
        "model, optimizer = init_train_model(\n",
        "    model_name_str=model_name_flag,\n",
        "    opt_name_str=optimizer_name_flag,\n",
        "    lr=learning_rate,\n",
        "    r_pim_hp=R_PIM,\n",
        "    alpha_pim_hp=ALPHA_PIM,\n",
        "    shallow_idx_hp=SHALLOW_FEATURE_IDX\n",
        ")\n",
        "model.to(device)\n",
        "wandb.watch(model, criterion, log=\"all\", log_freq=100)\n",
        "\n",
        "print(f\"Optimizer: {optimizer_name_flag}, Learning Rate: {learning_rate}\")\n",
        "if 'pim' in model_name_flag:\n",
        "    print(f\"PIM Hyperparameters: R_PIM={R_PIM}, ALPHA_PIM={ALPHA_PIM}, SHALLOW_IDX={SHALLOW_FEATURE_IDX}\")\n",
        "\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch_num in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch_num+1}/{epochs}\")\n",
        "    epoch_log_dict = {\"epoch\": epoch_num + 1}\n",
        "\n",
        "    current_train_loss, current_train_acc = 0.0, 0.0\n",
        "    if train_loader and len(train_loader) > 0 :\n",
        "        if 'pim' in model_name_flag:\n",
        "            current_train_loss, current_train_acc = train_one_epoch_with_pim(model, train_loader, optimizer, criterion, device, ALPHA_PIM, R_PIM)\n",
        "        else:\n",
        "            current_train_loss, current_train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Train Loss: {current_train_loss:.4f}, Acc: {current_train_acc:.4f}\")\n",
        "        epoch_log_dict[\"train/loss\"] = current_train_loss\n",
        "        epoch_log_dict[\"train/accuracy\"] = current_train_acc\n",
        "    else:\n",
        "        print(\"Skipping training for epoch due to empty or None train_loader.\")\n",
        "        epoch_log_dict[\"train/loss\"] = None\n",
        "        epoch_log_dict[\"train/accuracy\"] = None\n",
        "\n",
        "\n",
        "    current_val_loss, current_val_acc = 0.0, 0.0\n",
        "    if val_loader and len(val_loader) > 0:\n",
        "        current_val_loss, current_val_acc = evaluate(model, val_loader, criterion, device)\n",
        "        print(f\"Val   Loss: {current_val_loss:.4f}, Acc: {current_val_acc:.4f}\")\n",
        "        epoch_log_dict[\"val/loss\"] = current_val_loss\n",
        "        epoch_log_dict[\"val/accuracy\"] = current_val_acc\n",
        "    else:\n",
        "        print(\"Skipping validation for epoch due to empty or None val_loader.\")\n",
        "        epoch_log_dict[\"val/loss\"] = None\n",
        "        epoch_log_dict[\"val/accuracy\"] = None\n",
        "\n",
        "    train_losses.append(current_train_loss)\n",
        "    train_accs.append(current_train_acc)\n",
        "    val_losses.append(current_val_loss)\n",
        "    val_accs.append(current_val_acc)\n",
        "\n",
        "    wandb.log(epoch_log_dict)\n",
        "\n",
        "    if current_val_acc > best_val_acc and (val_loader and len(val_loader) > 0):\n",
        "        best_val_acc = current_val_acc\n",
        "        model_save_path_best = os.join.path(ROOT_MODEL_DIR/f'{model_name_flag}_best_val_acc.pth')\n",
        "        if device.type == 'cuda' and torch.cuda.device_count() > 1 and isinstance(model, nn.DataParallel):\n",
        "            torch.save(model.module.state_dict(), model_save_path_best)\n",
        "        else:\n",
        "            torch.save(model.state_dict(), model_save_path_best)\n",
        "        print(f\"Best model (Val Acc: {best_val_acc:.4f}) saved to {model_save_path_best}\")\n",
        "        wandb.save(model_save_path_best, base_path=ROOT_MODEL_DIR)\n",
        "\n",
        "\n",
        "if train_losses and val_losses and train_accs and val_accs:\n",
        "    fig_performance, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    axs[0].plot(train_losses, label='Train Loss')\n",
        "    axs[0].plot(val_losses, label='Val Loss')\n",
        "    axs[0].legend()\n",
        "    axs[0].set_title('Loss over epochs')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_ylabel('Loss')\n",
        "\n",
        "    axs[1].plot(train_accs, label='Train Acc')\n",
        "    axs[1].plot(val_accs, label='Val Acc')\n",
        "    axs[1].legend()\n",
        "    axs[1].set_title('Accuracy over epochs')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_ylabel('Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    wandb.log({\"Training Performance Plot\": wandb.Image(fig_performance)})\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping performance plot: Not enough data.\")\n",
        "\n",
        "\n",
        "model_save_path_final = os.join.path(ROOT_MODEL_DIR,f'{model_name_flag}_final_epochs{epochs}.pth')\n",
        "if device.type == 'cuda' and torch.cuda.device_count() > 1 and isinstance(model, nn.DataParallel):\n",
        "    torch.save(model.module.state_dict(), model_save_path_final)\n",
        "else:\n",
        "    torch.save(model.state_dict(), model_save_path_final)\n",
        "print(f\"Final model saved to {model_save_path_final}\")\n",
        "wandb.save(model_save_path_final, base_path=ROOT_MODEL_DIR)\n",
        "\n",
        "\n",
        "optimizer_save_path = os.join.path(ROOT_MODEL_DIR, f'{model_name_flag}_optimizer_final_epochs{epochs}.pth')\n",
        "torch.save(optimizer.state_dict(), optimizer_save_path)\n",
        "print(f\"Optimizer state saved to {optimizer_save_path}\")\n",
        "wandb.save(optimizer_save_path, base_path=ROOT_MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok3sildbldP8"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQcaQ4dylf1g"
      },
      "source": [
        "### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PCKxkL8i2oJ"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = None, None\n",
        "all_test_labels, all_test_preds_probs, all_test_preds_labels = None, None, None\n",
        "\n",
        "if test_loader and len(test_loader) > 0 :\n",
        "    test_loss, test_acc, all_test_labels, all_test_preds_probs, all_test_preds_labels = evaluate(\n",
        "        model, test_loader, criterion, device, return_preds_labels=True\n",
        "    )\n",
        "    print(f\"Test Loss (Clean): {test_loss:.4f}, Acc (Clean): {test_acc:.4f}\")\n",
        "    wandb.summary[\"test/clean_loss\"] = test_loss\n",
        "    wandb.summary[\"test/clean_accuracy\"] = test_acc\n",
        "\n",
        "    log_detailed_metrics(\n",
        "        all_test_labels,\n",
        "        all_test_preds_probs,\n",
        "        all_test_preds_labels,\n",
        "        section_prefix=\"test/clean_detailed\",\n",
        "        log_to_wandb=True\n",
        "    )\n",
        "else:\n",
        "    print(\"Test loader is empty or None. Skipping clean data evaluation.\")\n",
        "    wandb.summary[\"test/clean_loss\"] = \"N/A\"\n",
        "    wandb.summary[\"test/clean_accuracy\"] = \"N/A\"\n",
        "    wandb.summary[\"test/clean_detailed_accuracy\"] = \"N/A\"\n",
        "    wandb.summary[\"test/clean_detailed_precision\"] = \"N/A\"\n",
        "    wandb.summary[\"test/clean_detailed_recall\"] = \"N/A\"\n",
        "    wandb.summary[\"test/clean_detailed_f1_score\"] = \"N/A\"\n",
        "    wandb.summary[\"test/clean_detailed_auc_roc\"] = \"N/A\"\n",
        "    wandb.summary[\"test/clean_detailed_avg_confidence\"] = \"N/A\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PudWzWWlh6q"
      },
      "source": [
        "### Attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxFp20N4i5m5"
      },
      "outputs": [],
      "source": [
        "attack_configurations = [\n",
        "    {\"name\": \"FGSM_eps0.05\", \"attack_class\": torchattacks.FGSM, \"params\": {\"eps\": 0.05}},\n",
        "    {\"name\": \"FGSM_eps0.1\", \"attack_class\": torchattacks.FGSM, \"params\": {\"eps\": 0.1}},\n",
        "    {\"name\": \"FGSM_eps0.2\", \"attack_class\": torchattacks.FGSM, \"params\": {\"eps\": 0.2}},\n",
        "\n",
        "    {\"name\": \"PGD_Linf_eps4_steps7\", \"attack_class\": torchattacks.PGD, \"params\": {\"eps\": 4/255, \"alpha\": 1/255, \"steps\": 7, \"random_start\": True}},\n",
        "    {\"name\": \"PGD_Linf_eps8_steps10\", \"attack_class\": torchattacks.PGD, \"params\": {\"eps\": 8/255, \"alpha\": 2/255, \"steps\": 10, \"random_start\": True}},\n",
        "    {\"name\": \"PGD_Linf_eps0.1_steps20\", \"attack_class\": torchattacks.PGD, \"params\": {\"eps\": 0.1, \"alpha\": 0.01, \"steps\": 20, \"random_start\": True}},\n",
        "]\n",
        "\n",
        "all_results_list = []\n",
        "\n",
        "if test_acc is not None and test_loss is not None:\n",
        "    all_results_list.append({\n",
        "        \"Attack Method\": \"Clean Data (No Attack)\",\n",
        "        \"Parameters\": \"N/A\",\n",
        "        \"Adversarial Accuracy\": f\"{test_acc*100:.2f}%\",\n",
        "        \"Adversarial Loss\": f\"{test_loss:.4f}\"\n",
        "    })\n",
        "else:\n",
        "    print(\"Clean test data results not available, skipping for summary table.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Starting Adversarial Attack Comparison ---\")\n",
        "clean_labels_for_asr = None\n",
        "clean_preds_for_asr = None\n",
        "clean_acc_for_asr_calc = None\n",
        "\n",
        "if all_test_labels is not None and all_test_preds_labels is not None:\n",
        "    clean_labels_for_asr = all_test_labels\n",
        "    clean_preds_for_asr = all_test_preds_labels\n",
        "    clean_acc_for_asr_calc = test_acc\n",
        "elif test_loader and len(test_loader) > 0:\n",
        "    print(\"Re-evaluating on clean data to get base predictions for ASR calculation...\")\n",
        "    _, clean_acc_for_asr_calc, clean_labels_for_asr, _, clean_preds_for_asr = evaluate(\n",
        "        model, test_loader, criterion, device, return_preds_labels=True\n",
        "    )\n",
        "    if clean_labels_for_asr.numel() > 0 :\n",
        "        print(f\"Clean accuracy for ASR base: {clean_acc_for_asr_calc:.4f}\")\n",
        "    else:\n",
        "        print(\"Failed to get clean predictions for ASR.\")\n",
        "        clean_labels_for_asr = None\n",
        "else:\n",
        "    print(\"Cannot calculate ASR as clean data predictions are unavailable and test_loader is empty.\")\n",
        "\n",
        "\n",
        "if test_loader and len(test_loader) > 0:\n",
        "    for config in attack_configurations:\n",
        "        attack_name_safe = \"\".join(c if c.isalnum() else \"_\" for c in config['name'])\n",
        "        print(f\"\\nInitializing Attack: {config['name']}\")\n",
        "        print(f\"Parameters: {config['params']}\")\n",
        "\n",
        "        try:\n",
        "            current_model_for_attack = model.module if isinstance(model, nn.DataParallel) else model\n",
        "            current_attack_instance = config[\"attack_class\"](current_model_for_attack, **config[\"params\"])\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing attack {config['name']}: {e}\")\n",
        "            print(\"Skipping this attack configuration.\")\n",
        "            all_results_list.append({\n",
        "                \"Attack Method\": config['name'],\n",
        "                \"Parameters\": str(config['params']),\n",
        "                \"Adversarial Accuracy\": \"Error during init\",\n",
        "                \"Adversarial Loss\": \"Error during init\"\n",
        "            })\n",
        "            wandb.log({\n",
        "                f\"test_adv/{attack_name_safe}_accuracy\": None,\n",
        "                f\"test_adv/{attack_name_safe}_loss\": None,\n",
        "                f\"test_adv_ASR/{attack_name_safe}\": None,\n",
        "                f\"test_adv_perturb_L0/{attack_name_safe}\": None,\n",
        "                f\"test_adv_perturb_L2/{attack_name_safe}\": None,\n",
        "                f\"test_adv_perturb_Linf/{attack_name_safe}\": None\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        adv_acc, adv_loss, adv_orig_labels, adv_preds_probs, adv_preds_labels, \\\n",
        "        avg_l0, avg_l2, avg_linf = evaluate_on_adversarial(\n",
        "            model, test_loader, criterion, device,\n",
        "            current_attack_instance, config['name'], return_details=True\n",
        "        )\n",
        "\n",
        "        print(f\"Results for {config['name']}:\")\n",
        "        print(f\"  Adversarial Accuracy: {adv_acc*100:.2f}%\")\n",
        "        print(f\"  Adversarial Loss: {adv_loss:.4f}\")\n",
        "        print(f\"  Avg Perturbation L0: {avg_l0:.2f}, L2: {avg_l2:.4f}, Linf: {avg_linf:.4f}\")\n",
        "\n",
        "\n",
        "        all_results_list.append({\n",
        "            \"Attack Method\": config['name'],\n",
        "            \"Parameters\": str(config['params']),\n",
        "            \"Adversarial Accuracy\": f\"{adv_acc*100:.2f}%\",\n",
        "            \"Adversarial Loss\": f\"{adv_loss:.4f}\",\n",
        "            \"Avg L0 Perturb\": f\"{avg_l0:.2f}\",\n",
        "            \"Avg L2 Perturb\": f\"{avg_l2:.4f}\",\n",
        "            \"Avg Linf Perturb\": f\"{avg_linf:.4f}\"\n",
        "        })\n",
        "        log_dict_adv = {\n",
        "            f\"test_adv/{attack_name_safe}_robust_accuracy\": adv_acc,\n",
        "            f\"test_adv/{attack_name_safe}_loss\": adv_loss,\n",
        "            f\"test_adv_perturb_L0/{attack_name_safe}\": avg_l0,\n",
        "            f\"test_adv_perturb_L2/{attack_name_safe}\": avg_l2,\n",
        "            f\"test_adv_perturb_Linf/{attack_name_safe}\": avg_linf\n",
        "        }\n",
        "\n",
        "        log_detailed_metrics(\n",
        "            adv_orig_labels,\n",
        "            adv_preds_probs,\n",
        "            adv_preds_labels,\n",
        "            section_prefix=f\"test_adv_detailed/{attack_name_safe}\",\n",
        "            log_to_wandb=True\n",
        "        )\n",
        "\n",
        "        asr = \"N/A\"\n",
        "        if clean_labels_for_asr is not None and clean_preds_for_asr is not None and adv_orig_labels.numel() > 0:\n",
        "            if len(clean_labels_for_asr) == len(adv_orig_labels):\n",
        "                originally_correct_mask = (clean_preds_for_asr == clean_labels_for_asr)\n",
        "                adversarially_misclassified_mask = (adv_preds_labels != adv_orig_labels)\n",
        "\n",
        "                successful_attacks_mask = originally_correct_mask & adversarially_misclassified_mask\n",
        "\n",
        "                num_originally_correct = originally_correct_mask.sum().item()\n",
        "                num_successful_attacks = successful_attacks_mask.sum().item()\n",
        "\n",
        "                if num_originally_correct > 0:\n",
        "                    asr = num_successful_attacks / num_originally_correct\n",
        "                    print(f\"  Attack Success Rate (ASR): {asr*100:.2f}%\")\n",
        "                else:\n",
        "                    asr = 0.0\n",
        "                    print(f\"  Attack Success Rate (ASR): 0.0% (no samples were originally correct)\")\n",
        "            else:\n",
        "                print(f\"  ASR calculation skipped for {attack_name_safe}: Mismatch in sample count between clean ({len(clean_labels_for_asr)}) and adversarial ({len(adv_orig_labels)}) evaluations.\")\n",
        "        else:\n",
        "            print(f\"  ASR calculation skipped for {attack_name_safe}: Clean predictions or adversarial results unavailable.\")\n",
        "\n",
        "        log_dict_adv[f\"test_adv_ASR/{attack_name_safe}\"] = asr\n",
        "        wandb.log(log_dict_adv)\n",
        "\n",
        "\n",
        "    print(\"\\n--- Summary of All Attack Results ---\")\n",
        "    results_dataframe = pd.DataFrame(all_results_list)\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_colwidth', None, 'display.width', 1000):\n",
        "        print(results_dataframe)\n",
        "\n",
        "    wandb.log({\"Adversarial Attack Summary Table\": wandb.Table(dataframe=results_dataframe)})\n",
        "\n",
        "    csv_filename = f\"adversarial_attack_comparison_{wandb.run.name}.csv\"\n",
        "    results_dataframe.to_csv(csv_filename, index=False)\n",
        "    print(f\"\\nResults saved to {csv_filename}\")\n",
        "    wandb.save(csv_filename)\n",
        "\n",
        "    if len(all_results_list) > 1 and \"Error\" not in str(all_results_list[1].get(\"Adversarial Accuracy\", \"\")):\n",
        "        print(\"\\n--- Visualizing some adversarial examples ---\")\n",
        "\n",
        "        atk_vis_config = attack_configurations[0]\n",
        "        try:\n",
        "            current_model_for_attack_vis = model.module if isinstance(model, nn.DataParallel) else model\n",
        "            atk_vis_instance = atk_vis_config[\"attack_class\"](current_model_for_attack_vis, **atk_vis_config[\"params\"])\n",
        "\n",
        "            images_vis, labels_vis = next(iter(test_loader))\n",
        "            images_vis, labels_vis = images_vis.to(device), labels_vis.to(device)\n",
        "\n",
        "            adv_images_vis = atk_vis_instance(images_vis, labels_vis)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs_clean_vis = model(images_vis)\n",
        "                _, predicted_clean_vis = torch.max(outputs_clean_vis, 1)\n",
        "                outputs_adv_vis = model(adv_images_vis)\n",
        "                _, predicted_adv_vis = torch.max(outputs_adv_vis, 1)\n",
        "\n",
        "            num_to_show = min(5, images_vis.size(0))\n",
        "\n",
        "            inv_normalize = transforms.Normalize(\n",
        "                mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "                std=[1/0.229, 1/0.224, 1/0.225]\n",
        "            )\n",
        "            wandb_images_log = {}\n",
        "            for i in range(num_to_show):\n",
        "                clean_img_display = inv_normalize(images_vis[i].cpu())\n",
        "                adv_img_display = inv_normalize(adv_images_vis[i].cpu())\n",
        "                perturbation_vis = adv_img_display - clean_img_display\n",
        "                perturbation_vis_scaled = (perturbation_vis - perturbation_vis.min()) / (perturbation_vis.max() - perturbation_vis.min() + 1e-6)\n",
        "\n",
        "\n",
        "                fig_vis, axs_vis = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "                axs_vis[0].imshow(np.transpose(clean_img_display.numpy(), (1,2,0)))\n",
        "                axs_vis[0].set_title(f\"Clean - True: {labels_vis[i].item()}, Pred: {predicted_clean_vis[i].item()}\")\n",
        "                axs_vis[0].axis('off')\n",
        "\n",
        "                axs_vis[1].imshow(np.transpose(adv_img_display.numpy(), (1,2,0)))\n",
        "                axs_vis[1].set_title(f\"Adv ({atk_vis_config['name']}) - True: {labels_vis[i].item()}, Pred: {predicted_adv_vis[i].item()}\")\n",
        "                axs_vis[1].axis('off')\n",
        "\n",
        "                axs_vis[2].imshow(np.transpose(perturbation_vis_scaled.numpy(), (1,2,0)))\n",
        "                axs_vis[2].set_title(f\"Perturbation (Scaled)\")\n",
        "                axs_vis[2].axis('off')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                wandb_images_log[f\"Adversarial_Example_{i}/Clean_Img\"] = wandb.Image(clean_img_display, caption=f\"Clean - True: {labels_vis[i].item()}, Pred: {predicted_clean_vis[i].item()}\")\n",
        "                wandb_images_log[f\"Adversarial_Example_{i}/Adv_Img_{atk_vis_config['name']}\"] = wandb.Image(adv_img_display, caption=f\"Adv ({atk_vis_config['name']}) - True: {labels_vis[i].item()}, Pred: {predicted_adv_vis[i].item()}\")\n",
        "                wandb_images_log[f\"Adversarial_Example_{i}/Perturbation_{atk_vis_config['name']}\"] = wandb.Image(perturbation_vis_scaled, caption=\"Perturbation (Scaled)\")\n",
        "            if wandb_images_log:\n",
        "                 wandb.log(wandb_images_log)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Could not generate or log visualizations for attack {atk_vis_config['name']}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "else:\n",
        "    print(\"Test loader is empty or None. Skipping adversarial attack evaluation and visualization.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLqm-lAKIVhk"
      },
      "outputs": [],
      "source": [
        "wandb.finish()\n",
        "print(\"WandB run finished.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "V9KseWpOjmRP",
        "ELzvoShxkZ9T",
        "G--s6Ezdkfar",
        "mUAOms1hknsP",
        "h8ZfVdCgk1yp",
        "7kq8aCpdB5RZ",
        "qQcaQ4dylf1g",
        "8PudWzWWlh6q"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
